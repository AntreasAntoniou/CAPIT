{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def upload_file(files):\n",
    "    file_paths = [file.name for file in files]\n",
    "    return file_paths\n",
    "\n",
    "def check_if_folder_has_images(file_paths):\n",
    "    return len(file_paths) > 0\n",
    "\n",
    "def show_state_of_files(file_paths):\n",
    "    print(file_paths.value)\n",
    "    \n",
    "def rank(images, prompt):\n",
    "    # randomize order of images\n",
    "    images = [file.name for file in images]\n",
    "    return np.random.permutation(images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gate/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from capit.models import *\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "class Ranker(nn.Module):\n",
    "    def __init__(self, model_name_or_path, pretrained):\n",
    "        super().__init__()\n",
    "        self.model = CLIPImageTextModel(pretrained=pretrained, \n",
    "                                    model_name_or_path=model_name_or_path)\n",
    "        self.model = self.model.to(torch.cuda.current_device())\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.pretrained = pretrained\n",
    "        \n",
    "    def rank(self, image_paths, prompt):\n",
    "        with torch.no_grad():\n",
    "            images = [transforms.ToTensor()(Image.open(file.name)) for file in image_paths]\n",
    "            similarities = self.model.forward(image=images, text=prompt)\n",
    "            rank_similarities_args = torch.argsort(similarities.logits_per_image, descending=True)[0]\n",
    "            return [image_paths[i].name for i in rank_similarities_args]\n",
    "\n",
    "class CAPITRanker(Ranker):\n",
    "    def __init__(self, model_name_or_path, ckpt_path):\n",
    "        super().__init__(model_name_or_path, False)\n",
    "        model_weight = torch.load(ckpt_path)\n",
    "        state_dict = {}\n",
    "        for key, value in model_weight[\"state_dict\"].items():\n",
    "            state_dict[key.replace(\"model.model.\", \"model.\")] = value\n",
    "\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model = self.model.to(torch.cuda.current_device())\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "import gradio as gr\n",
    "\n",
    "def build_demo(model_dict, collection_num_images: int = 0, challenge_num_images: int = 5):\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=224):\n",
    "                prompt = gr.Textbox(label=\"prompt\", value=\"\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=224):\n",
    "                rank_status = gr.Button(value=\"rank\", label=\"rank\")\n",
    "        with gr.Row():\n",
    "            if collection_num_images > 0:\n",
    "                collection_images = []\n",
    "                with gr.Column(scale=collection_num_images, min_width=224):\n",
    "                    for i in range(collection_num_images):\n",
    "                        collection_images.append(gr.Image(shape=(224, 224), label=f\"collection-image-{i}\"))\n",
    "            with gr.Column(scale=challenge_num_images, min_width=224):\n",
    "                challenge_images = []\n",
    "                for i in range(challenge_num_images):\n",
    "                    challenge_images.append(gr.Image(shape=(224, 224), label=f\"challenge-image-{i}\"))\n",
    "            ranked_images_dict = defaultdict(list)\n",
    "            for key, model in model_dict.items():\n",
    "                with gr.Column(scale=challenge_num_images, min_width=224):\n",
    "                    for i in range(challenge_num_images):\n",
    "                        ranked_images_dict[key].append(gr.Image(shape=(224, 224), label=f\"ranked-image-{key}-{i}\"))\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2, min_width=224):\n",
    "                upload_button = gr.UploadButton(\"Browse to select a folder with images\", file_types=[\"image\"], file_count=\"multiple\")\n",
    "                upload_button.upload(upload_file, upload_button, challenge_images)\n",
    "                for model_name, model in model_dict.items():\n",
    "                    rank_status.click(fn=model.rank, inputs=[upload_button, prompt], outputs=ranked_images_dict[model_name])\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random = Ranker(\"openai/clip-vit-large-patch14\", pretrained=False)\n",
    "model_random = model_random.to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = Ranker(\"openai/clip-vit-large-patch14\", pretrained=True)\n",
    "model_baseline = model_baseline.to(torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = CAPITRanker(\"openai/clip-vit-large-patch14\", ckpt_path=\"/workspaces/CAPMultiModal-1/capit-clip-ft/last.ckpt\")\n",
    "model_fine_tuned = model_fine_tuned.to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f83dbf85238f7aac.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f83dbf85238f7aac.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dict = {\"baseline\": model_baseline, \"fine-tuned\": model_fine_tuned}\n",
    "num_challenge_images = 25\n",
    "num_collection_images = 0\n",
    "demo = build_demo(challenge_num_images=num_challenge_images, model_dict=model_dict)\n",
    "demo.queue()\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "378cc278b42c1eceffcac57679d483e09c256f28d4a65ec194c01bb14cb8d12e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
